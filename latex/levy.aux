\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\citation{gatto2015numerical,unser2009multiresolution}
\citation{scalas2000fractional}
\citation{bogdan2003censored}
\citation{zaslavsky2002chaos}
\citation{alibaud2012continuous}
\citation{chen2010anomalous,chen2006speculative,epps2018turbulence}
\citation{lischke2018fractional}
\citation{bonito2018numerical,huang2016finite}
\citation{meidner2017hp,kyprianou2017unbiased,acosta2018regularity,zhao2017adaptive}
\citation{garbaczewski2018fractional}
\citation{barndorff2012levy}
\HyPL@Entry{0<</S/D>>}
\providecommand\csxdef[2]{}
\@writefile{toc}{\providecommand\autonum@processReference[2]{}}
\@writefile{lof}{\providecommand\autonum@processReference[2]{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{kwasnicki2017ten}
\citation{MichaelC25:online}
\citation{bebendorf2008hierarchical,borm2003introduction}
\citation{golub2012matrix}
\citation{borm2003introduction,bebendorf2008hierarchical}
\citation{pouransari2017fast}
\citation{metropolis1989monte}
\citation{MichaelC25:online}
\citation{hammersley2013monte}
\newlabel{equ:levy}{{1}{2}{}{equation.1.1}{}}
\newlabel{equ:levy@cref}{{[equation][1][]1}{2}}
\csxdef {autonum@equ:levyReferenced}{}
\csxdef {autonum@equ:levyReferenced}{}
\csxdef {autonum@equ:levyReferenced}{}
\csxdef {autonum@equ:levyReferenced}{}
\csxdef {autonum@equ:levyReferenced}{}
\csxdef {autonum@equ:levyReferenced}{}
\citation{doney2007introduction,papapantoleon2008introduction,sato1999levy}
\@writefile{toc}{\contentsline {section}{\numberline {2}L\'evy Process}{3}{section.2}}
\newlabel{equ:eta}{{2}{3}{}{equation.2.2}{}}
\newlabel{equ:eta@cref}{{[equation][2][]2}{3}}
\newlabel{equ:f}{{3}{4}{}{equation.2.3}{}}
\newlabel{equ:f@cref}{{[equation][3][]3}{4}}
\csxdef {autonum@equ:etaReferenced}{}
\csxdef {autonum@equ:fReferenced}{}
\citation{sun2012fokker}
\citation{borm2003introduction,bebendorf2008hierarchical,yang2008construction}
\@writefile{toc}{\contentsline {section}{\numberline {3}$\mathcal  {H}$ Matrix}{5}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Construction and Storage}{5}{subsection.3.1}}
\newlabel{equ:cons}{{3.1}{5}{Construction and Storage}{subsection.3.1}{}}
\newlabel{equ:cons@cref}{{[subsection][1][3]3.1}{5}}
\newlabel{equ:adm}{{4}{5}{}{equation.3.4}{}}
\newlabel{equ:adm@cref}{{[equation][4][]4}{5}}
\csxdef {autonum@equ:admReferenced}{}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Matrix Vector Multiplication}{7}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}LU Decomposition}{7}{subsection.3.3}}
\citation{MichaelC25:online}
\citation{cont2005finite}
\@writefile{toc}{\contentsline {section}{\numberline {4}Crank Nicolson Scheme Based on $\mathcal  {H}$-matrix}{8}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Model Problem}{8}{subsection.4.1}}
\newlabel{equ:ut}{{5}{8}{Model Problem}{equation.4.5}{}}
\newlabel{equ:ut@cref}{{[equation][5][]5}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Numerical Scheme}{8}{subsection.4.2}}
\newlabel{equ:small}{{6}{8}{Numerical Scheme}{equation.4.6}{}}
\newlabel{equ:small@cref}{{[equation][6][]6}{8}}
\newlabel{equ:dl}{{7}{9}{Numerical Scheme}{equation.4.7}{}}
\newlabel{equ:dl@cref}{{[equation][7][]7}{9}}
\csxdef {autonum@equ:utReferenced}{}
\newlabel{equ:scheme}{{8}{9}{Numerical Scheme}{equation.4.8}{}}
\newlabel{equ:scheme@cref}{{[equation][8][]8}{9}}
\newlabel{equ:Aij}{{9}{9}{Numerical Scheme}{equation.4.9}{}}
\newlabel{equ:Aij@cref}{{[equation][9][]9}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}$\mathcal  {H}$-matrix Construction}{9}{subsection.4.3}}
\csxdef {autonum@equ:AijReferenced}{}
\csxdef {autonum@equ:dlReferenced}{}
\csxdef {autonum@equ:AijReferenced}{}
\newlabel{equ:r}{{10}{10}{}{equation.4.10}{}}
\newlabel{equ:r@cref}{{[equation][10][]10}{10}}
\newlabel{equ:alphabeta}{{11}{10}{}{equation.4.11}{}}
\newlabel{equ:alphabeta@cref}{{[equation][11][]11}{10}}
\csxdef {autonum@equ:rReferenced}{}
\csxdef {autonum@equ:rReferenced}{}
\csxdef {autonum@equ:rReferenced}{}
\csxdef {autonum@fig:constructionReferenced}{}
\citation{isaacson2012analysis}
\csxdef {autonum@equ:alphabetaReferenced}{}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The construction time and the storage consumption of $\mathcal  {H}$-matrix. We compare the construction time of the $\mathcal  {H}$-matrix with that of the dense matrix. We can see that the construction of $\mathcal  {H}$-matrix is quite efficient, both in terms of storage consumption and time consumption: they both achieves an approximately linear asymptotic rate with respect to the problem size $N$.}}{12}{figure.1}}
\newlabel{fig:construction}{{1}{12}{The construction time and the storage consumption of $\mathcal {H}$-matrix. We compare the construction time of the $\mathcal {H}$-matrix with that of the dense matrix. We can see that the construction of $\mathcal {H}$-matrix is quite efficient, both in terms of storage consumption and time consumption: they both achieves an approximately linear asymptotic rate with respect to the problem size $N$}{figure.1}{}}
\newlabel{fig:construction@cref}{{[figure][1][]1}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Error Analysis}{13}{subsection.4.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Stability}{13}{subsubsection.4.4.1}}
\newlabel{equ:von}{{12}{13}{Stability}{equation.4.12}{}}
\newlabel{equ:von@cref}{{[equation][12][]12}{13}}
\citation{giles2005convergence}
\csxdef {autonum@equ:vonReferenced}{}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Consistency}{14}{subsubsection.4.4.2}}
\csxdef {autonum@equ:smallReferenced}{}
\newlabel{equ:assumption}{{13}{14}{Consistency}{equation.4.13}{}}
\newlabel{equ:assumption@cref}{{[equation][13][]13}{14}}
\csxdef {autonum@equ:dlReferenced}{}
\newlabel{equ:deltal}{{14}{14}{Consistency}{equation.4.14}{}}
\newlabel{equ:deltal@cref}{{[equation][14][]14}{14}}
\newlabel{equ:deltar}{{15}{14}{Consistency}{equation.4.15}{}}
\newlabel{equ:deltar@cref}{{[equation][15][]15}{14}}
\csxdef {autonum@equ:deltalReferenced}{}
\csxdef {autonum@equ:deltarReferenced}{}
\citation{giles2005convergence}
\newlabel{lemma:consistency}{{3}{15}{Consistency}{lemma.3}{}}
\newlabel{lemma:consistency@cref}{{[lemma][3][]3}{15}}
\csxdef {autonum@equ:assumptionReferenced}{}
\csxdef {autonum@equ:schemeReferenced}{}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}Convergence}{15}{subsubsection.4.4.3}}
\csxdef {autonum@equ:schemeReferenced}{}
\csxdef {autonum@lemma:consistencyReferenced}{}
\csxdef {autonum@equ:schemeReferenced}{}
\@writefile{toc}{\contentsline {section}{\numberline {5}Singular and Slow Decaying L\'evy Measure}{15}{section.5}}
\newlabel{sect:extension}{{4.4.3}{15}{Convergence}{section.5}{}}
\newlabel{sect:extension@cref}{{[subsubsection][3][4,4]4.4.3}{15}}
\newlabel{equ:Ix}{{16}{15}{Convergence}{equation.5.16}{}}
\newlabel{equ:Ix@cref}{{[equation][16][]16}{15}}
\newlabel{equ:rho_condition}{{17}{15}{Convergence}{equation.5.17}{}}
\newlabel{equ:rho_condition@cref}{{[equation][17][]17}{15}}
\newlabel{equ:ny}{{18}{16}{Convergence}{equation.5.18}{}}
\newlabel{equ:ny@cref}{{[equation][18][]18}{16}}
\csxdef {autonum@equ:rho_conditionReferenced}{}
\csxdef {autonum@equ:rho_conditionReferenced}{}
\csxdef {autonum@equ:IxReferenced}{}
\citation{minden2018simple}
\newlabel{equ:fxy}{{19}{17}{Convergence}{equation.5.19}{}}
\newlabel{equ:fxy@cref}{{[equation][19][]19}{17}}
\csxdef {autonum@equ:IxReferenced}{}
\newlabel{equ:evalIx1d}{{20}{17}{Convergence}{equation.5.20}{}}
\newlabel{equ:evalIx1d@cref}{{[equation][20][]20}{17}}
\csxdef {autonum@equ:fxyReferenced}{}
\csxdef {autonum@equ:nyReferenced}{}
\csxdef {autonum@equ:evalIx1dReferenced}{}
\csxdef {autonum@equ:nyReferenced}{}
\newlabel{equ:I1approx}{{21}{18}{Convergence}{equation.5.21}{}}
\newlabel{equ:I1approx@cref}{{[equation][21][]21}{18}}
\newlabel{equ:I2approx}{{22}{18}{Convergence}{equation.5.22}{}}
\newlabel{equ:I2approx@cref}{{[equation][22][]22}{18}}
\newlabel{equ:I3approx}{{23}{18}{Convergence}{equation.5.23}{}}
\newlabel{equ:I3approx@cref}{{[equation][23][]23}{18}}
\csxdef {autonum@tab:1dReferenced}{}
\citation{tankov2009jump,kou2002jump}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces To compute $I(x_i)$, we need to compute the near field interaction and local interaction using values of $u(x)$ from the green area. The values of $u(x)$ are provided in the green and red area for computing $I(x)$, $x\in [-L, L]$.}}{19}{figure.2}}
\newlabel{fig:fig31}{{2}{19}{To compute $I(x_i)$, we need to compute the near field interaction and local interaction using values of $u(x)$ from the green area. The values of $u(x)$ are provided in the green and red area for computing $I(x)$, $x\in [-L, L]$}{figure.2}{}}
\newlabel{fig:fig31@cref}{{[figure][2][]2}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Applications}{19}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Option Pricing}{19}{subsection.6.1}}
\citation{laskin2010principles,hasan2018tunneling,garbaczewski1995schrodinger,laskin2000fractional}
\citation{chen2010anomalous,chen2006speculative}
\citation{epps2018turbulence}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Quantum Mechanics}{20}{subsection.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Turbulence Flow}{21}{subsection.6.3}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Numerical Examples}{21}{section.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Efficiency of $\mathcal  {H}$-Matrix: 1D Case}{21}{subsection.7.1}}
\csxdef {autonum@fig:1DReferenced}{}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces We use a different color for each block. The blue and green blocks represent low rank blocks while the yellow blocks represent dense blocks.}}{22}{figure.3}}
\newlabel{fig:1D}{{3}{22}{We use a different color for each block. The blue and green blocks represent low rank blocks while the yellow blocks represent dense blocks}{figure.3}{}}
\newlabel{fig:1D@cref}{{[figure][3][]3}{22}}
\csxdef {autonum@fig:compressReferenced}{}
\@writefile{toc}{\contentsline {paragraph}{Matrix Vector Multipliction}{22}{section*.1}}
\@writefile{toc}{\contentsline {paragraph}{LU Decomposition}{22}{section*.2}}
\csxdef {autonum@fig:constructionReferenced}{}
\csxdef {autonum@fig:luReferenced}{}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Compression ratio and number of blocks as the problem size grows.}}{23}{figure.4}}
\newlabel{fig:compress}{{4}{23}{Compression ratio and number of blocks as the problem size grows}{figure.4}{}}
\newlabel{fig:compress@cref}{{[figure][4][]4}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Matrix vector production is also much more efficient using the $\mathcal  {H}$-matrices than using the dense matrix. It has the asymptotic complexity rate slightly above $\mathcal  {O}(N)$, compared to $\mathcal  {O}(N^2)$ for dense matrices.}}{23}{figure.5}}
\newlabel{fig:matvec}{{5}{23}{Matrix vector production is also much more efficient using the $\mathcal {H}$-matrices than using the dense matrix. It has the asymptotic complexity rate slightly above $\mathcal {O}(N)$, compared to $\mathcal {O}(N^2)$ for dense matrices}{figure.5}{}}
\newlabel{fig:matvec@cref}{{[figure][5][]5}{23}}
\@writefile{toc}{\contentsline {paragraph}{Solve}{23}{section*.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces LU decomposition of $\mathcal  {H}$-LU and the dense LU. The $\mathcal  {H}$-LU has better asymptotic complexity than the dense LU, which has complexity $\mathcal  {O}(N^3)$.}}{24}{figure.6}}
\newlabel{fig:lu}{{6}{24}{LU decomposition of $\mathcal {H}$-LU and the dense LU. The $\mathcal {H}$-LU has better asymptotic complexity than the dense LU, which has complexity $\mathcal {O}(N^3)$}{figure.6}{}}
\newlabel{fig:lu@cref}{{[figure][6][]6}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Efficiency of $\mathcal  {H}$-Matrix: 2D Case}{24}{subsection.7.2}}
\newlabel{equ:k}{{24}{24}{Efficiency of $\mathcal {H}$-Matrix: 2D Case}{equation.7.24}{}}
\newlabel{equ:k@cref}{{[equation][24][]24}{24}}
\csxdef {autonum@fig:2dReferenced}{}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Solving time for both the dense matrix and the $\mathcal  {H}$-matrix. The $\mathcal  {H}$-matrix solving is both faster and has better asymptotic rate than the dense one.}}{25}{figure.7}}
\newlabel{fig:solve}{{7}{25}{Solving time for both the dense matrix and the $\mathcal {H}$-matrix. The $\mathcal {H}$-matrix solving is both faster and has better asymptotic rate than the dense one}{figure.7}{}}
\newlabel{fig:solve@cref}{{[figure][7][]7}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces A typical $\mathcal  {H}$-matrix in 2D constructed from \autonum@processReference {autonum@referencecrefOld}{forcsvlist}{equ:k}.}}{25}{figure.8}}
\csxdef {autonum@equ:kReferenced}{}
\newlabel{fig:2d}{{8}{25}{A typical $\mathcal {H}$-matrix in 2D constructed from \cref {equ:k}}{figure.8}{}}
\newlabel{fig:2d@cref}{{[figure][8][]8}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Comparison of construction time, matrix vector multiplication time, LU decomposition, and solving for both dense matrices as well as $\mathcal  {H}$-matrices. The $\mathcal  {H}$-matrix technique has better asymptotic rate than that of the dense matrices in terms of time consumption. The green dashed line shows the theoretical complexity asymptotic rate for dense matrices.}}{26}{figure.9}}
\newlabel{fig:solve}{{9}{26}{Comparison of construction time, matrix vector multiplication time, LU decomposition, and solving for both dense matrices as well as $\mathcal {H}$-matrices. The $\mathcal {H}$-matrix technique has better asymptotic rate than that of the dense matrices in terms of time consumption. The green dashed line shows the theoretical complexity asymptotic rate for dense matrices}{figure.9}{}}
\newlabel{fig:solve@cref}{{[figure][9][]9}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Convection Diffusion Equation Driven by the L\'evy Process}{26}{subsection.7.3}}
\@writefile{toc}{\contentsline {paragraph}{Verification Method}{26}{section*.4}}
\csxdef {autonum@equ:utReferenced}{}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Time and storage complexity for $\mathcal  {H}$-matrix and full matrix. Both in terms of time and storage complexity $\mathcal  {H}$-matrix has advantage over full matrices for large scale problems.}}{27}{figure.10}}
\newlabel{fig:s}{{10}{27}{Time and storage complexity for $\mathcal {H}$-matrix and full matrix. Both in terms of time and storage complexity $\mathcal {H}$-matrix has advantage over full matrices for large scale problems}{figure.10}{}}
\newlabel{fig:s@cref}{{[figure][10][]10}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Singular and Slow Decaying L\'evy Measure}{27}{subsection.7.4}}
\newlabel{equ:u0}{{25}{27}{Singular and Slow Decaying L\'evy Measure}{equation.7.25}{}}
\newlabel{equ:u0@cref}{{[equation][25][]25}{27}}
\csxdef {autonum@equ:I1approxReferenced}{}
\csxdef {autonum@equ:I2approxReferenced}{}
\csxdef {autonum@equ:I3approxReferenced}{}
\csxdef {autonum@equ:u0Referenced}{}
\csxdef {autonum@fig:fig5Referenced}{}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Numerical error for approximating \autonum@processReference {autonum@referencecrefOld}{forcsvlist}{equ:u0}. The error converges like or better than $\mathcal  {O}(h^2)$.}}{28}{figure.11}}
\csxdef {autonum@equ:u0Referenced}{}
\newlabel{fig:fig5}{{11}{28}{Numerical error for approximating \cref {equ:u0}. The error converges like or better than $\mathcal {O}(h^2)$}{figure.11}{}}
\newlabel{fig:fig5@cref}{{[figure][11][]11}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The finite difference result obtained from our discretization. The convergence order is $1.0$ or less, much worse than the Poisson problem where $\mathcal  {O}(h^2)$ convergence rate is typical}}{29}{figure.12}}
\newlabel{fig:fig6}{{12}{29}{The finite difference result obtained from our discretization. The convergence order is $1.0$ or less, much worse than the Poisson problem where $\mathcal {O}(h^2)$ convergence rate is typical}{figure.12}{}}
\newlabel{fig:fig6@cref}{{[figure][12][]12}{29}}
\newlabel{equ:u1}{{26}{29}{Singular and Slow Decaying L\'evy Measure}{equation.7.26}{}}
\newlabel{equ:u1@cref}{{[equation][26][]26}{29}}
\csxdef {autonum@fig:fig8Referenced}{}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Numerical evaluation of \autonum@processReference {autonum@referencecrefOld}{forcsvlist}{equ:u1}. Near the boundary, due to the nonsmoothness of $u(\mathbf  {x})$, the algorithm is having a hard time computing the nonlocal gradient and therefore we see the oscillatory behavior. However, the computation for the region near the center is good, which does not suffer much from the far-away contribution from nonsmooth boundaries. In the center the error is only $4\%$.}}{30}{figure.13}}
\csxdef {autonum@equ:u1Referenced}{}
\newlabel{fig:fig8}{{13}{30}{Numerical evaluation of \cref {equ:u1}. Near the boundary, due to the nonsmoothness of $u(\bx )$, the algorithm is having a hard time computing the nonlocal gradient and therefore we see the oscillatory behavior. However, the computation for the region near the center is good, which does not suffer much from the far-away contribution from nonsmooth boundaries. In the center the error is only $4\%$}{figure.13}{}}
\newlabel{fig:fig8@cref}{{[figure][13][]13}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{30}{section.8}}
\bibstyle{unsrt}
\bibdata{levy.bib}
\bibcite{gatto2015numerical}{{1}{}{{}}{{}}}
\bibcite{unser2009multiresolution}{{2}{}{{}}{{}}}
\bibcite{scalas2000fractional}{{3}{}{{}}{{}}}
\bibcite{bogdan2003censored}{{4}{}{{}}{{}}}
\bibcite{zaslavsky2002chaos}{{5}{}{{}}{{}}}
\bibcite{alibaud2012continuous}{{6}{}{{}}{{}}}
\bibcite{chen2010anomalous}{{7}{}{{}}{{}}}
\bibcite{chen2006speculative}{{8}{}{{}}{{}}}
\bibcite{epps2018turbulence}{{9}{}{{}}{{}}}
\bibcite{lischke2018fractional}{{10}{}{{}}{{}}}
\bibcite{bonito2018numerical}{{11}{}{{}}{{}}}
\bibcite{huang2016finite}{{12}{}{{}}{{}}}
\bibcite{meidner2017hp}{{13}{}{{}}{{}}}
\bibcite{kyprianou2017unbiased}{{14}{}{{}}{{}}}
\bibcite{acosta2018regularity}{{15}{}{{}}{{}}}
\bibcite{zhao2017adaptive}{{16}{}{{}}{{}}}
\bibcite{garbaczewski2018fractional}{{17}{}{{}}{{}}}
\bibcite{barndorff2012levy}{{18}{}{{}}{{}}}
\bibcite{kwasnicki2017ten}{{19}{}{{}}{{}}}
\bibcite{MichaelC25:online}{{20}{}{{}}{{}}}
\bibcite{bebendorf2008hierarchical}{{21}{}{{}}{{}}}
\bibcite{borm2003introduction}{{22}{}{{}}{{}}}
\bibcite{golub2012matrix}{{23}{}{{}}{{}}}
\bibcite{pouransari2017fast}{{24}{}{{}}{{}}}
\bibcite{metropolis1989monte}{{25}{}{{}}{{}}}
\bibcite{hammersley2013monte}{{26}{}{{}}{{}}}
\bibcite{doney2007introduction}{{27}{}{{}}{{}}}
\bibcite{papapantoleon2008introduction}{{28}{}{{}}{{}}}
\bibcite{sato1999levy}{{29}{}{{}}{{}}}
\bibcite{sun2012fokker}{{30}{}{{}}{{}}}
\bibcite{yang2008construction}{{31}{}{{}}{{}}}
\bibcite{cont2005finite}{{32}{}{{}}{{}}}
\bibcite{isaacson2012analysis}{{33}{}{{}}{{}}}
\bibcite{giles2005convergence}{{34}{}{{}}{{}}}
\bibcite{minden2018simple}{{35}{}{{}}{{}}}
\bibcite{tankov2009jump}{{36}{}{{}}{{}}}
\bibcite{kou2002jump}{{37}{}{{}}{{}}}
\bibcite{laskin2010principles}{{38}{}{{}}{{}}}
\bibcite{hasan2018tunneling}{{39}{}{{}}{{}}}
\bibcite{garbaczewski1995schrodinger}{{40}{}{{}}{{}}}
\bibcite{laskin2000fractional}{{41}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
