\documentclass[10pt,a4paper]{article}
%\usepackage{minted}

\usepackage{amsmath}
\usepackage{amsthm}

\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{todonotes}
\newcommand{\ii}[0]{\mathrm{i}}
\newcommand{\RR}[0]{\mathbb{R}}
\newcommand{\bx}[0]{\mathbf{x}}
\newcommand{\bw}[0]{\mathbf{w}}
\newcommand{\bu}[0]{\mathbf{u}}
\newcommand{\bC}[0]{\mathbf{C}}
\newcommand{\bF}[0]{\mathbf{F}}
\newcommand{\bI}[0]{\mathbf{I}}
\newcommand{\bA}[0]{\mathbf{A}}
\newcommand{\bbf}[0]{\mathbf{f}}
\newcommand{\lib}[1]{\textcolor{blue}{\section{#1}}}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
 
 
\usepackage{pgfplots}


\newcommand{\by}[0]{\mathbf{y}}

\newcommand{\sinc}[0]{\mathrm{sinc}}
\usepackage{amsthm}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}

\newtheorem{remark}{Remark}

\title{Efficient Numerical Method for Models Driven by L\'evy Process via Hierarchical Matrices}
\date{}
%\author{Kailai Xu}
\begin{document}

\maketitle
\begin{abstract}
	The modeling via the fractional partial differential equation or the L\'evy process has been an active area of research and has many applications. However, lack of efficient numerical computation method impede people from adopting such a modeling tool. We proposed an efficient solver for convection diffusion equation whose operator is the infinitesimal generator of a L\'evy process based on $\mathcal{H}$-matrix technique. The proposed Crank Nicolson scheme is unconditionally stable and has theoretical $\mathcal{O}(h^2+\Delta t^2)$ convergence rate. The $\mathcal{H}$-matrix technique has theoretical $\mathcal{O}(N)$ space and computational complexity compared to $\mathcal{O}(N^2)$ and $\mathcal{O}(N^3)$ respectively for the direct method. Numerical experiments demonstrate the efficiency of the new algorithm. 
\end{abstract}

\lib{Introduction}
Over the last years anomalous diffusion or nonlocal modeling have seen a tremendous increase in popularities in many fields. Of particular interest is the fractional partial differential equations~(FPDE) arising from many disciplines such as ... One extensively studied fractional operator is the fractional Laplacian 
\begin{equation}
	(-\Delta)^s u(\bx)=\mathrm{r.v.}\int_{\RR^d} c_{d,s}\frac{u(\bx+\by)-u(\bx)}{|\by|^{d+2s}}d\bx
\end{equation}
which is considered as the natural generalization of the Laplacian operator to the normal Laplacian operator. However, the numerical computation of the FPDE with such operators exhibits particular difficulties: (1) the kernel function $\frac{c_{d,s}}{|\by|^{d+2s}}$ has singularities which must be dealt with special care; (2) the kernel function is nonlocal, and therefore the corresponding coefficient matrix is typically dense. The second difficulty impedes people from using the new modeling tool due to its prohibitable computational requirement. There are some efforts to speed up the computation, mainly through analyzing its special structure or modifying the definition. 

Taking another point of view, the fractional partial differential equation with the Laplacian operator~(and many others) can be derived from the infinitesimal generator of the L\'evy process. Indeed, the forward equation~(or Fokker Planck equation in physics) will have the form
\begin{equation}\label{equ:levy}
	u_t = a u_{xx} + bu_x + cu + \mathcal{L}u\quad x\in \RR, t\in (0,1)
\end{equation}
where $a\geq 0$, $c\leq 0$, $b\in\RR$, and
\begin{equation}
	\mathcal{L}u = \int_\RR (u(x+y)-u(x)-u'(x)\mathbf{1}_{0<|y|<1}(y)y)\nu(y)dy
\end{equation}  
Here $\nu(y)$ will be a proper L\'evy measure. The fractional Laplacian is just a special case where~(d=1) $\nu(y) = \frac{c_{1,s}}{|y|^{1+2s}}$. The model \cref{equ:levy} incorporates a much richer structure and has a broader of applications. For example, in recent years, the modeling of financial markets by L\'evy process has become an active area of research. The numerical difficulties are similar to that of FPDE. 

In this paper, we aim at solving \cref{equ:levy} efficiently based on the well-established $\mathcal{H}$-matrix technique. In principal, our algorithm can work for any $\nu(y)$ under mild assumptions, including singular or slow decaying L\'evy measure. We focus on the efficiency of the operator since the ability to efficient store data and solve is the main bottleneck for today's applications. In particular, the algorithm will equivalently work for many FPDE models, on condition it can be written in the form of \cref{equ:levy}. 

The advantage of adopting the $\mathcal{H}$-matrix is its high efficiency. If direct method is used, which results in a dense coefficient matrix, the storage complexity will be $\mathcal{O}(N^2)$ while the computation complexity will be $\mathcal{O}(N^3)$~(LU factorization). However, theoretically, $\mathcal{H}$-matrix can achieve nearly optimal $\mathcal{O}(N)$ storage and computation complexity. Although in practice, the optimal rate is hard to achieve~(due to the increasing rank during LU factorization), we will demonstrate with careful implementation, the new algorithm has great advantage over the direct method for medium and large scale problems. 

We mention that there are other approaches to solve FPDE. One of the main numerical methods is the Monte Carlo methods, which is based on the probabilistic interpretation of the model. Let $X_t$ be a L\'evy process with the L\'evy measure $\nu(y)$ and appropriate diffusion and drift coefficients, under certain assumptions, the solution to \cref{equ:levy} can be written as
\begin{equation}
	u(x) = \mathbb{E}(u(X_t)|X_0=x)
\end{equation}
and Monte Carlo method can be applied thereafter. Although Monte Carlo might be the only way possible to numerically compute the solution in high dimensions, it suffers from slow convergence and therefore impractical for some cases. The grid-based method, such as the one we proposed in the paper, will enjoy fast convergence~(and we will prove that the convergence order is $\mathcal{O}(\Delta t^2+h^2)$). 

To end this section, we summarize our major contributions of the paper
\begin{itemize}
	\item Proposed and analyzed an unconditional stable Crank Nicolson scheme for the model problem \cref{equ:levy}. The theoretical error is $\mathcal{O}(\Delta t^2 + h^2)$.
	\item Proposed and implemented an efficient solver for \cref{equ:levy} based on  $\mathcal{H}$-matrix technique. The theoretical space and computation complexity is $\mathcal{O}(N)$. 
	\item Proposed a trick for computation involving L\'evy measure that has singular and slow decaying L\'evy measure.
\end{itemize}



\lib{L\'evy Process}

Consider a given probability space $(\Omega, \mathcal{F}, P)$. A L\'evy process $\{X_t\}_{t\geq 0}$ taking values in $\RR^d$ is defined as a stochastic process with stationary and independent increments. In addition, we assume $X_0=0$ with probability 1. 

By independent, we mean for any distinct time $0\leq t_1<t_2<\ldots<t_n$, we have $X_{t_1}, X_{t_2-t_1}, \ldots, X_{t_n}-X_{t_{n-1}}$ are all independent. 

By stationary,  for any $0\leq s < t <\infty$, the probability distribution of $X_t-X_s$ is the same as $X_{t-s}$.

One remarkable property of the L\'evy process is that any L\'evy process has a specific form of the characteristic function, called \textit{L\'evy-Khintchine formula}
\begin{equation}
	\mathbb{E}(e^{\ii (\xi, X_t)}) = e^{t\eta(\xi)}
\end{equation}
where
\begin{equation}\label{equ:eta}
	\eta(\xi) = \ii (b, \xi) - \frac{1}{2}(\xi, a\xi) + \int_{\RR^d\backslash \{0\}} \left[ e^{\ii (\xi, y)}-1-\ii (\xi, y)\mathbf{1}[0<|y|<1](y) \right]dy
\end{equation}
here $b\in \RR^d$, $a$ is a positive definite symmetric matrix in $\RR^{d\times d}$, and $\nu$ is a L\'evy measure which satisfies
\begin{equation}
	\int_{\RR^d\backslash \{0\}} \min\{1, |y|^2\}\nu(dy)<\infty
\end{equation}

In the case $\nu\equiv 0$, we obtain the Gaussian process. In the case $\int_{\RR^d} \left[ e^{\ii (\xi, y)}-1\right]dy$ is well defined, we can omit the term $\ii (\xi, y)\mathbf{1}[0<|y|<1](y)$. 

In the case $\nu<\infty$, the L\'evy process has the decomposition
\begin{equation}
	X_t = bt + \sqrt{a}B_t + \sum_{0\leq s\leq t} J_s 
\end{equation}
where $J_s$ is the jump at time $s$. To be precise, define 
\begin{equation}
	N(t, A) = \#\{0\leq s\leq t: J_s\in A\}
\end{equation}
if $t$ and $A$ is fixed, $N(t, A)$ is a random variable; if $t$ and $w\in \Omega$ is fixed, $N(t, \cdot)(w)$ is a measure; if $A$ is fixed, $N(\cdot, A)$ is a Poisson process with intensity $\nu(A)$. Therefore, we can also write
\begin{equation}
	\sum_{0\leq s\leq t} J_s = \int_{\RR^d-\{0\}} xN(t, dx)
\end{equation}

To end this section, we provide a third view of the L\'evy process. Consider the semigroup 
\begin{equation}
	(T_tf)(x) = \mathbb{E}(f(X_t+x))
\end{equation}
Then the infinitesimal generator will have the form
\begin{multline}\label{equ:Af}
	(Af)(x) =  b^i(\partial_i f)(x) + \frac{1}{2}a^{ij}(\partial_i \partial_j f)(x) +\\
	\int_{\RR^d\backslash\{0\}} [f(x+y)-f(x)-y^i(\partial_i f)(x) \mathbf{1}_{0<|y|<1}(y)]\nu(dy)
\end{multline}

\begin{remark}
	Another definition of the infinitesimal generator is through the Fourier transform 
	\begin{equation}
		(Af)(x) = \lim_{t\rightarrow 0+} \frac{P_t f-f}{t}
	\end{equation}
	where $P_tf = f\star p_t$ and $\mathcal{F}p_t(\xi) = e^{-t\eta(\xi)}$. 
	
	To see this, consider the 1D case and without the adjustment term $-y^i(\partial_i f)(x) \mathbf{1}_{0<|y|<1}(y)$. By taking the Fourier transform of $(Af)(x)$, we have 
	\begin{multline}\label{equ:f}
		\mathcal{F}(Af)(\xi) = \ii\xi b\hat f(\xi ) - \frac{1}{2}a{\xi ^2}\hat f(\xi ) + \int_{ - \infty }^{ + \infty } {(\hat f(\xi ){e^{\ii y\xi }} - \hat f(\xi ))\nu (y)dy} \\
		=\left( {\ii\xi b - \frac{1}{2}a{\xi ^2} + \int_{ - \infty }^{ + \infty } {({e^{\ii y\xi }} - 1)\nu (y)dy} } \right)\hat f(\xi )
	\end{multline}
	this is exactly the expression we see in \cref{equ:eta}. 
	
	One the other hand, 
	\[\mathcal{F}\left[\mathop {\lim }\limits_{t \to 0 + } \frac{{{P_t}f - f}}{t}\right] = \mathop {\lim }\limits_{t \to 0 + } \frac{{{e^{\eta (\xi )t}}\hat f(\xi ) - \hat f(\xi )}}{t} = \eta (\xi )\]
	which coincides with \cref{equ:f}. 
	
\end{remark}

Again, we can drop the extra term $y^i(\partial_i f)(x) \mathbf{1}_{0<|y|<1}(y)$ if $\int_{\RR^d} [f(x+y)-f(x)]\nu(dy)$ is well defined. We call the L\'evy measure is \textit{regular} if this condition is fulfilled.


Let $T_t$ be the semigroup associated with L\'evy process, and the associated infinitesimal generator is
\begin{multline}\label{equ:Af2}
	(Af)(x) =  c(x) f(x) + b^i(\partial_i f)(x) + \frac{1}{2}a^{ij}(\partial_i \partial_j f)(x) +\\
	\int_{\RR^d\backslash\{0\}} [f(x+y)-f(x)-y^i(\partial_i f)(x) \mathbf{1}_{0<|y|<1}(y)]\nu(dy)
\end{multline}

 we consider the transition measures $p_t(x)$ associated with $T_t$. Here $p_t$ is absolutely continuous with respect to Lebesgue measure. Define the adjoint operator $A^*$ of $A$, which satisfies
 \begin{equation}
 	\int_{\RR^d} (Af)(y) p_t(y)dy=\int_{\RR^d} f(y)A^*p_t(y)dy
 \end{equation}
 for all $f\in C_c^\infty(\RR^d)$.
 
\url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.750.9443&rep=rep1&type=pdf}


 In general, there is no nice form for $A^*$. However, in the case $c(x)$, $b(x)$ and $a(x)$ are all constant, we have
 \begin{multline}
 	A^*p_t(x) = c (\partial_i p_t)(x) - b^i (\partial_i p_t)(x) + \frac{1}{2}a^{ij}\partial_i\partial_j p_t +\\
 	\int_{\RR^d}\left[ p_t( x-y) -p_t(x)+y^i(\partial_i p_t)(x) \mathbf{1}_{0<|y|<1}(y) \right]\nu(dy)
 \end{multline}
 
 The Fokker-Planck equation, or Kolmogorov forward equation, is
 \begin{equation}
 	\frac{\partial p_t(x)}{\partial t} = A^*p_t(x)\quad p_0(x) = \delta(x)
 \end{equation}
 
 
 
\lib{$\mathcal{H}$ Matrix}

For completeness, we discuss the hierarchical matrix technique. Especially we give a detailed description on the storage format, construction, fast matrix vector multiplication routine and LU decomposition. We later show how to construct the $\mathcal{H}$ matrix from kernels. 

The discretization of the jump diffusion part $\int_\RR (u(x+y)-u(y))dy$ will usually lead to a dense matrix, which typically requires $\mathcal{O}(N^2)$ storage and has $\mathcal{O}(N^2)$ complexity for matrix-vector multiplication, $\mathcal{O}(N^3)$ for LU decomposition. Many techniques, such as the panel clustering methods and the fast multipole methods were developed. Later $\mathcal{H}$-matrix was considered by W.~Hackbusch and many variations of hierarchical matrices have been intensively studied by researchers. $\mathcal{H}$-matrices can reduces the storage and arithmetics to nearly optimal complexity $\mathcal{O}(N)$ up to $\log N$ scaling. It relies on the fact that the kernel functions are smooth in the off diagonal. 

\subsection{Construction and Storage}\label{equ:cons}

The construction of the $\mathcal{H}$ matrices can be best described in terms of matrix indices and the geometric points. Each entry $A_{ij}$ represents the interaction between two nodes $x_i$ and $x_j$. Let $I$, $J\subset \mathbb{N}$ be row and column index sets, then $A_{IJ}=(a_{ij})_{i\in I, j\in J}$ describes the interaction between a cluster $X_I = \{x_i\}_{i\in I}$ and another cluster $X_J = \{x_j\}_{j\in I}$. The interaction kernel function $k(x,y)$ is assumed to be smooth for sufficiently large $|x-y|$. 

Typically, it requires $\mathcal{O}(|I||J|)$ complexity to store the interaction data. However, if we assume that $I\subset J=\emptyset$ and geometrically the clusters $X_I$, $X_J$ are separate in the sense of admissibility

\begin{definition}
 For two sets of indices $I$ and $J$ and the associated cluster $X_I$, $X_J$; assume that the kernel is asymptotically smooth, the admissibility condition is given by
	\begin{equation}\label{equ:adm}
		\min\{\mathrm{diam}(X_I),\mathrm{diam}(X_J) \}\leq \eta \mathrm{dist}(X_I,X_J)
	\end{equation}
	where $\mathrm{diam}(X_I) = \max_{x_i,x_j\in X}|x_i-x_j|$ AND $\mathrm{dist}(x_i,x_j)=\min_{x_i\in X_I, x_j\in X_J}|x_i-x_j|$. If the condition \cref{equ:adm} is not satisfied, we say $X_I$ and $X_J$ or $I$ and $J$ are inadmissible. 
\end{definition}

In our numerical examples, we use $\eta=1$, which indicates adjacent clusters are inadmissible since the distance is always zero. 

The admissible blocks usually have low rank structures. This is best illustrated by an example. Suppose $k(x,y)=\frac{1}{|x-y|^2}$, and further assume $x\in X_I$, $y\in X_J$. Assume $X_I$ and $Y_J$ are inadmissible, and $\bar x\in \mathcal{X}$, where $\mathcal{X}$ is the convex hull of $X_I$. Then we have
\[\frac{1}{{|x - y{|^2}}} = \frac{1}{{|x - \bar x - (y - \bar x){|^2}}} = \frac{1}{{|y - \bar x{|^2}{{\left| {\frac{{x - \bar x}}{{y - \bar x}} - 1} \right|}^2}}}\]

Since 
\[\left| {\frac{{x - \bar x}}{{y - \bar x}}} \right| < 1\]
we have 
\[\frac{1}{{|y - \bar x{|^2}{{\left| {\frac{{x - \bar x}}{{y - \bar x}} - 1} \right|}^2}}} = \frac{1}{{|y - \bar x{|^2}}}\left( {1 + \frac{{x - \bar x}}{{y - \bar x}} + {{\left( {\frac{{x - \bar x}}{{y - \bar x}}} \right)}^2} +  \ldots } \right)\]
which is in the form of 
\begin{equation}
	\frac{1}{{|x - y{|^2}}} = \sum_{n=0}^\infty \alpha_n(x-\bar x)\beta_n(y-\bar x)
\end{equation}

Then series is convergent and therefore the residual term will decay. It is possible to approximate $\frac{1}{{|x - y{|^2}}}$ with a few terms
\begin{equation}
	\frac{1}{{|x - y{|^2}}} \approx \sum_{n=0}^r \alpha_n(x-\bar x)\beta_n(y-\bar x)
\end{equation}
And therefore the interaction matrix for the cluster $A_{IJ}$ is 
\begin{equation}
	A_{IJ} = \left(\frac{1}{{|x_i - x_j{|^2}}}\right)_{i\in I, j\in J} =\left( \sum_{n=0}^r \alpha_n(x_i-\bar x)\beta_n(x_j-\bar x)\right)_{i\in I, j\in J} = UV'
\end{equation}
where
\begin{equation}
	U = \begin{bmatrix}
		\alpha_0(x_i-\bar x)& \alpha_1(x_i-\bar x)&\ldots & \alpha_r(x_i-\bar x)
	\end{bmatrix}
\end{equation}
\begin{equation}
	V = \begin{bmatrix}
		\beta_0(x_i-\bar x)& \beta_1(x_i-\bar x)&\ldots & \beta_r(x_i-\bar x)
	\end{bmatrix}
\end{equation}
If $r\ll |I|\wedge |J|$, we have achieved matrix compression using a low rank representation. 

The idea of the hierarchical matrix is then to classify each block $A_{IJ}$ into three types
\begin{itemize}
\item Full matrix. In this case, $A_{IJ}$ is represented using fully populated matrices.
\item Low rank matrix. In the case $I$ and $J$ are admissible, we can store the block $A_{IJ}$ in the form of low rank matrices. This will help us save storage and computational cost.
	\item $\mathcal{H}$-matrix. For the blocks that are neither low rank matrix nor small enough to become a full matrix, it is further divided into subblocks~(for example, via quadtree structure). 
\end{itemize}

The $\mathcal{H}$-matrix will be stored in a hierarchical format. 


\subsection{Matrix Vector Multiplication}

One advantage of the $\mathcal{H}$ matrix is that the matrix vector multiplication is cheap. The matrix vector multiplication of $\mathcal{H}$-matrix can be described through the rule of the operator for three different kinds of subblocks
\begin{itemize}
	\item Full matrix. In this case, the normal dense matrix vector multiplication is used. 
	\item Low rank matrix. The operator can be carried out quite efficiently via
	\begin{equation}
		(UV')x = U(V'x)
	\end{equation}
	note $V'x$ is a $r\times 1$ vector.
	\item $\mathcal{H}$-matrix. If the subblock is 
	\begin{equation}
		B = \begin{bmatrix}
			B_{11}&B_{12}\\
			B_{21}&B_{22}
		\end{bmatrix}\quad x = \begin{bmatrix}
			x_1\\
			x_2
		\end{bmatrix}
	\end{equation}
	the matrix vector multiplication will be carried out recursively, i.e.
	\begin{equation}
		Bx = \begin{bmatrix}
			B_{11} x_1 + B_{12}x_2\\
			B_{21} x_1 + B_{22}x_2
		\end{bmatrix}
	\end{equation}
\end{itemize}





\subsection{LU Decomposition}

$\mathcal{H}$-LU can be done in $\mathcal{H}$-matrix format and recursively in computational cost $\mathcal{O}(N)$ up to a $\log N$ scaling compared to dense LU in $\mathcal{O}(N^3)$.


We need to define a triangular solver which solves $AX=B$ for lower triangular matrix or $XA = B$ or upper triangular matrix. The matrices are either $\mathcal{H}$-matrix or full matrix. We only need to consider the lower triangular cases since in the latter case by transposition $A'X'=B'$, we reduce the problem to former. 

The triangular solver will work differently for different situations.
\begin{itemize}
	\item If $B$ is a full matrix, then $X$ is a full matrix and $X=A^{-1}B$. Here $A$ is converted to a full matrix.
	\item If $B$ is a low rank matrix, $B=B_1B_2'$, then $X$ is also a low rank matrix $X = (A^{-1}B_1)B_2'$.
	\item If $A$ and $B$ are both hierarchical matrices
	\[\left[ {\begin{array}{*{20}{c}}
{{A_{11}}}&{}\\
{{A_{21}}}&{{A_{22}}}
\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
{{X_{11}}}&{{X_{12}}}\\
{{X_{21}}}&{{X_{22}}}
\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}
{{B_{11}}}&{{B_{12}}}\\
{{B_{21}}}&{{B_{22}}}
\end{array}} \right]\]
Then we will first solve $A_{11}X_{11}=B_{11}$ and $A_{11}X_{12}=B_{12}$. Then we solve
\begin{equation}
	A_{22}X_{21} = B_{21}-A_{21}X_{11}\quad A_{22}X_{22} = B_{22}-A_{21}X_{12}
\end{equation}	
\end{itemize}

The LU decomposition also works differently for different types of matrices. Again only full matrices and $\mathcal{H}$ matrices are considered. 

For full matrices, the standard dense LU is adopted. For $\mathcal{H}$ matrices, 
\[\left[ {\begin{array}{*{20}{c}}
{{A_{11}}}&{{A_{12}}}\\
{{A_{21}}}&{{A_{22}}}
\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}
{{L_{11}}}&\\
{{L_{21}}}&{{L_{22}}}
\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
{{U_{11}}}&{{U_{12}}}\\
{}&{{U_{22}}}
\end{array}} \right]\]

The algorithm will works as follows
\begin{itemize}
	\item LU decomposition of $A_{11}=L_{11}U_{11}$
	\item Triangular solve $L_{11}U_{12}=A_{12}$~(lower triangular, $U_{12}$ is the unknown)
	\item Triangular solve $L_{21}U_{11} = A_{21}$~(upper triangular, $L_{21}$ is the unknown)
	\item LU decomposition of $A_{22}-L_{21}U_{12}=L_{22}U_{22}$
\end{itemize}	

The LU decomposition can also be performed in an in-place way, which will save storage. 

 
\lib{Crank Nicolson Scheme Based on $\mathcal{H}$-matrix} 
 
 
 \subsection{Model Problem}
 We will consider the forward or backward equation driven by the L\'evy process, where the model problem in 1D can be stated as a convection-diffusion integro-differential equation
\begin{equation}\label{equ:ut}
	u_t = a u_{xx} + bu_x + cu + \mathcal{L}u\quad x\in \RR, t\in (0,1)
\end{equation}
where $a\geq 0$, $c\leq 0$, $b\in\RR$, and
\begin{equation}
	\mathcal{L}u = \int_\RR (u(x+y)-u(x)-u'(x)\mathbf{1}_{0<|y|<1}(y)y)\nu(y)dy
\end{equation}  


 \subsection{Numerical Scheme}

We consider the case where $\nu(y)<\infty$ and thus the term $u'(x)\mathbf{1}_{0<|y|<1}(y)y$ is not needed. In addition, we assume $\nu(y)$ is semi-heavy.   The case $\nu(y)$ might grow to infinity at $y=0$ and decays algebraically will be discussed in \Cref{sect:extension}. To compute numerically the integral term, we need to restrict the computational domain to a bounded interval $\Omega$
\begin{equation}
	\mathcal{L}u \approx \int_{B_l}^{B_r} (u(x+y)-u(x)-u'(x)\mathbf{1}_{0<|y|<1}(y)y)\nu(y)dy
\end{equation}

In fact, it is proved in ??? that if $\nu(dy)$ is semi-heavy, i.e., there exists $\alpha_r,\alpha_l>0$, such that $\int_1^\infty e^{(1+\alpha_r)y}\nu(dy)<\infty$, and $\int_{-\infty}^{-1}|y|e^{\alpha_l|y|}\nu(dy)<\infty$, the solution $\tilde u(x,t)$ obtained using the truncated integral will satisfy
\begin{equation}\label{equ:small}
	|u( x,t)-\tilde u(x,t)| =\mathcal{O}(e^{-\alpha_l|B_l|}+e^{-\alpha_r|B_r|})
\end{equation}
Therefore, the discretization scheme for $\mathcal{L}$ using trapezoidal rule on uniform grid will be
\begin{equation}
	(\mathcal{L} u)(jh) \approx   \sum_{j\in \mathcal{I}} u_{i+j}\nu_j w_j - u_i \lambda w_j\quad \lambda = \sum_{j\neq 0, j\in \mathcal{I}} \nu_j
\end{equation}
Here $\nu_j=\nu(jh)$, 
\begin{equation}
	\mathcal{I} = \{i: ih\in \Omega\}
\end{equation}
$w_j$ is the weight for the trapezoidal rule and 
\begin{equation}
	w_j = \begin{cases}
		h & j \mbox{ is not the endpoint of } \mathcal{I}\\
		\frac{h}{2} & j \mbox{ is the endpoint of } \mathcal{I}
	\end{cases}
\end{equation}
We define the discrete operator $\delta_L$
\begin{equation}\label{equ:dl}
	(\delta_L u)_j = \sum_{j=-\infty}^\infty (u_{i+j}-u_i)\nu_j h =  \sum_{j\in \mathcal{I}, j=-\infty}^\infty u_{i+j}\nu_j h - u_i \lambda h\quad \lambda = \sum_{j\neq 0, j\in \mathcal{I}} \nu_j
\end{equation}
Then Crank-Nicolson discretization of \cref{equ:ut} on a uniform grid with spacing $h$ and timestep $\Delta t$ is 
\begin{equation}\label{equ:scheme}
	(I+\frac{1}{2}\Delta tA)u_j^{n+1} = (I-\frac{1}{2}\Delta tA)u_j^n
\end{equation}
where 
\begin{equation}
	A = -a\delta_x^2 - b\delta_{2x} - c - \delta_L
\end{equation}
here $\delta_x^2$ and $\delta_{2x}$ are the standard second difference and central first difference. 
Therefore, we have
\begin{equation}\label{equ:Aij}
  {A_{ij}} = 
  \begin{cases}
  	{\frac{{2a}}{{{h^2}}} - c + \lambda w_{j-i}} & i=j\\
{ - \frac{a}{{{h^2}}} + \frac{b}{{2h}} - {\nu _{ - 1}w_{j-i}}} & j=i-1\\
{ - \frac{a}{{{h^2}}} - \frac{b}{{2h}} - {\nu _1}w_{j-i}} & j=i+1\\
{ - {\nu _{j - i}w_{j-i}}} & |j-i|\geq 2
  \end{cases}
\end{equation}
 
 \subsection{$\mathcal{H}$-matrix Construction} 
 
 For simplicity, assume $a=b=c=0$; according to \cref{equ:Aij}, these coefficients only contribute to the first off-diagonal parts of the coefficient matrix. We consider the matrix $A_\pm = I \pm \frac{1}{2}A$. Note since the operator $\delta_x^2$, $\delta_{2x}$ only contributes to the tridiagonal, any nonzero entry in $A_\pm$ in the off-diagonal more than one entry away from the diagonal must be $\mp\frac{1}{2}\nu_jh$ according to \cref{equ:dl}. 

We define the kernel associated with each L\'evy measure by
\begin{equation}
	K(x,y) = \nu(y-x)
\end{equation}
then we have $A_{i'j'}=K(x_{i},x_{j})$ for $|i-j|\geq 2$, where $i'$ and $j'$ corresponds to $x_{i}$, $x_j$.



 The numerical scheme lead us to the following task
 \begin{align}
 	\mbox{Given $\bx$, compute $\by$: }&\by = (I+\lambda A)\bx\\
 	\mbox{Given $\by$, solve for $\bx$: }&\by = (I+\lambda A)\bx
 \end{align}
 here $\lambda>0$ is a constant. We illustrate here the application of the $\mathcal{H}$-matrix technique using the example of $A$ generated by Merton matrix. Assume $a=c=0$ in \cref{equ:Aij}, then we can see that
 \begin{equation}
 	A_{ij} = h^d k(x_i, x_j)
 \end{equation}
 for some kernel function $k(x,y)$. 
 
Consider the Merton model with Gaussian jumps, i.e., the L\'evy density can be represented as
\begin{equation}
	\nu(x) = e^{-\varepsilon^2 x^2}
\end{equation}

We consider the kernel function $k(x,y)$ associated with the density
\begin{equation}
	k(x,y) = \nu(x-y) = e^{-\varepsilon^2 (x-y)^2}
\end{equation}
Assume that $x\in \mathcal{X}$, $y\in \mathcal{Y}$, and $\mathcal{X} \cap \mathcal{Y}=\emptyset$, and let $\bar x\in\mathcal{X}$. Denote $t_0=x-\bar x$, and $t=y-\bar x$, then by assumption we have $|t|>|t_0|$. From Taylor expansion we have
\begin{align}
	{e^{ - {\varepsilon ^2}{{(x - y)}^2}}} =& {e^{ - {\varepsilon ^2}{{(t - {t_0})}^2}}}\\
	=&  {e^{ - {\varepsilon ^2}{t^2}{{\left( {1 - \frac{{{t_0}}}{t}} \right)}^2}}} = {e^{ - {\varepsilon ^2}{t^2} - {\varepsilon ^2}t_0^2 + 2{\varepsilon ^2}{t_0}t}}\\
	=& {e^{ - {\varepsilon ^2}{t^2} - {\varepsilon ^2}t_0^2}}\left( {1 + 2{\varepsilon ^2}{t_0}t + \frac{{{{(2{\varepsilon ^2}{t_0}t)}^2}}}{2} + \frac{{{{(2{\varepsilon ^2}{t_0}t)}^3}}}{{3!}} +  \ldots } \right)
\end{align} 
Thus we have
\begin{equation}
	\alpha_n(t) = \frac{{{2^n}{\varepsilon ^{2n}}{e^{ - {\varepsilon ^2}t^2}}t^n}}{{n!}}\quad \beta_n(t) = {e^{ - {\varepsilon ^2}{t^2}}}{t^n}
\end{equation}
we will have
\begin{equation}
	{e^{ - {\varepsilon ^2}{{(x - y)}^2}}} =\sum_{n=0}^\infty \alpha_n(t_0)\beta_n(t)
\end{equation}

\begin{lemma}
	Assume $\mathcal{X}$, $\mathcal{Y}$ are two disjoint set in $\Omega$ and $\mathrm{diam}(\Omega)=D$. Let $\delta>0$ be any positive constant, then if 
	\begin{equation}\label{equ:r}
		r > \max\left\{ \log_2\left( \frac{e^{2\varepsilon^2D^2}}{\delta} \right)-1, 12\varepsilon^2D^2-1  \right\}
	\end{equation}
	 we have
	\begin{equation}\label{equ:alphabeta}
		|{e^{ - {\varepsilon ^2}{{(x - y)}^2}}}-\sum_{n=0}^r \alpha_n(x-\bar x)\beta_n(y-\bar x)|<\delta 
	\end{equation}
	for any $\bar x\in \mathcal{X}$. 
\end{lemma}

\begin{proof}
	Note that 
	\begin{align}
		&|{e^{ - {\varepsilon ^2}{{(x - y)}^2}}} - \sum\limits_{n = 0}^r {{\alpha _n}} (x - \bar x){\beta _n}(y - \bar x)|\\
		 =&  {e^{ - {\varepsilon ^2}{t^2} - {\varepsilon ^2}t_0^2}}\left( {\frac{{{{(2{\varepsilon ^2}{t_0}t)}^{n + 1}}}}{{(n + 1)!}} + \frac{{{{(2{\varepsilon ^2}{t_0}t)}^{n + 2}}}}{{(n + 2)!}} +  \ldots } \right)\\
		\le& {e^{ - {\varepsilon ^2}{t^2} - {\varepsilon ^2}t_0^2}}\frac{{{{(2{\varepsilon ^2}{t_0}t)}^{n + 1}}}}{{(n + 1)!}}{e^{2{\varepsilon ^2}{t_0}t}} \le {e^{2{\varepsilon ^2}{D^2}}}\frac{{{{(2{\varepsilon ^2}{D^2})}^{n + 1}}}}{{(n + 1)!}}
	\end{align}
	We invoke the basic estimate
	\begin{equation}
		n!>\left( \frac{n}{3} \right)^{n}
	\end{equation} 
	and obtain
	\begin{equation}
		|{e^{ - {\varepsilon ^2}{{(x - y)}^2}}} - \sum\limits_{n = 0}^r {{\alpha _n}} (x - \bar x){\beta _n}(y - \bar x)|\leq {e^{2{\varepsilon ^2}{D^2}}}\frac{{{{(2{\varepsilon ^2}{D^2})}^{n + 1}}}}{{{{\left( {\frac{{n + 1}}{3}} \right)}^{n + 1}}}} = {\left( {\frac{{6{\varepsilon ^2}{D^2}}}{{n + 1}}} \right)^{n + 1}}{e^{2{\varepsilon ^2}{D^2}}}
	\end{equation}
	
	Since we have \cref{equ:r}, which indicates 
	\begin{equation}
		{\frac{{6{\varepsilon ^2}{D^2}}}{{n + 1}}}<\frac{1}{2}
	\end{equation}
	and therefore
	\begin{equation}
		|{e^{ - {\varepsilon ^2}{{(x - y)}^2}}} - \sum\limits_{n = 0}^r {{\alpha _n}} (x - \bar x){\beta _n}(y - \bar x)| < {\left( {\frac{1}{2}} \right)^{n + 1}}{e^{2{\varepsilon ^2}{D^2}}} < \delta 
	\end{equation}
	the last equation is due to the assumption \cref{equ:r}.
\end{proof}

\begin{remark}
	In practice, the estimate \cref{equ:r} is quite conservative and smaller $r$ can actually work very well. 
\end{remark}

By using the $\mathcal{H}$-matrix, the storage complexity is reduced to $\mathcal{O}(N)$ which is demonstrated in \cref{fig:construction}. The construction time is also reduced to $\mathcal{O}(N)$ compared to $\mathcal{O}(N^2)$ for full matrices. 


In 2D, the Merton model with Gaussian jumps read
\begin{equation}
	\nu(\bx) = \exp(-\varepsilon^2 \|\bx\|^2)
\end{equation}
with the kernel function
\begin{equation}
	k(\bx, \by) = \nu(\bx-\by) = \exp(-\varepsilon^2 \|\bx-\by\|^2)
\end{equation}

Let $\bx\in\mathcal{X}$, $\by\in\mathcal{Y}$ and $\mathcal{X}\cap \mathcal{Y} = \emptyset$, and assume that $\bar x\in \mathcal{X}$, 
\begin{equation}
	t_1 = \bx_1-\bar\bx_1\quad t_2 = \bx_2-\bar\bx_2\quad s_1 = \by_1-\bar\by_1\quad s_2 = \by_2-\bar\by_2\quad
\end{equation}
we have
\begin{equation}
	k(\bx,\by) = \sum\limits_{m,n = 0}^\infty  {\left[ {\frac{{{{(2{\varepsilon ^2})}^{m + n}}}}{{m!n!}}s_1^mt_1^n\exp \left( { - {\varepsilon ^2}(t_1^2 + s_1^2)} \right)} \right]} \left[ {s_2^mt_2^n\exp \left( { - {\varepsilon ^2}(t_2^2 + s_2^2)} \right)} \right]
\end{equation}

Let 
\begin{align}
	\alpha_{m,n} &= {\frac{{{{(2{\varepsilon ^2})}^{m + n}}}}{{m!n!}}s_1^mt_1^n\exp \left( { - {\varepsilon ^2}(t_1^2 + s_1^2)} \right)}\\
	\beta_{m,n} &= {s_2^mt_2^n\exp \left( { - {\varepsilon ^2}(t_2^2 + s_2^2)} \right)}
\end{align}
Similar to \cref{equ:alphabeta}, we can approximate the kernel using low rank summation
\begin{equation}
	k(\bx,\by) \approx \sum_{m,n=0}^{r} \alpha_{m,n}(s_1,t_1)\beta_{m,n}(s_2,t_2)
\end{equation}

Using the storage strategy in \Cref{equ:cons}, we are able to construct the $\mathcal{H}$-matrix directly. \Cref{fig:construction} shows the construction time as well as the storage consumption. Particularly, we compare the construction time of the $\mathcal{H}$-matrix with that of the dense matrix. We can see that the construction of $\mathcal{H}$-matrix is quite efficient, both in terms of storage consumption and time consumption: they both achieves an approximately linear asymptotic rate with respect to the problem size $N$. For 16k$\times$16k matrices, the speed of constructing an $\mathcal{H}$-matrix is almost 600 times faster than the direct approach. 

\begin{figure}[htpb]
\centering
\scalebox{0.4}{\input{figures/icme_construction_time.pgf}}~
\scalebox{0.4}{\input{figures/icme_construction_storage.pgf}}
\caption{The construction time and the storage consumption of $\mathcal{H}$-matrix. We compare the construction time of the $\mathcal{H}$-matrix with that of the dense matrix. We can see that the construction of $\mathcal{H}$-matrix is quite efficient, both in terms of storage consumption and time consumption: they both achieves an approximately linear asymptotic rate with respect to the problem size $N$.}
\label{fig:construction}
\end{figure}



\subsection{Error Analysis}

\subsubsection{Stability}
We carry out the stability analysis using the Fourier transform pair
\begin{align}
	u_j^n =& \frac{1}{2\pi h}\int_{-\pi}^\pi \hat u^n(x) \exp(\ii j x)dx\\
	\hat u_j^n =&  h \sum_{-\infty}^\infty u_j^n \exp(-\ii jx)dx
\end{align}

For simplicity, we assume that the domain is not truncated, i.e. $\mathcal{I}=\mathbb{Z}$. We need the following formula for 
\begin{lemma}
	Let 
	\begin{equation}
		\eta_h(\theta) = \sum_{j=-\infty}^\infty (e^{\ii jh\theta}-1)v_jh
	\end{equation}
	then we have
	\begin{equation}
		\delta_L e^{\ii \theta x} = \eta_h(\theta) e^{\ii \theta x}
	\end{equation}
	where $\nu_j = \nu(jh)$. In the case $\nu_j$ is symmetric, we have
	\begin{equation}
		\eta_h(\theta) = -2\sum_{j=-\infty}^\infty \sin^2\left(\frac{j\theta h}{2} \right) v_jh\leq 0
	\end{equation}
\end{lemma}
\begin{proof}
	By definition, we have
	\[{\delta _L}{e^{\ii\theta x}} = \int_{ - \infty }^\infty  {\left( {{e^{\ii(x + y)\theta }} - {e^{\ii x\theta }}} \right)\nu (y)dy = {\eta _h}(\xi )} {e^{\ii\theta x}}\]
	The other properties easily follow. 
\end{proof}

The Fourier transform of the numerical scheme gives
\begin{equation}\label{equ:von}
	\hat u_i^{n+1} = \frac{{1 - a\frac{{\Delta t}}{{{h^2}}}{{\sin }^2}\frac{\theta }{2} + \frac{{b\Delta t}}{{2h}}\ii\sin \theta  + \frac{{c\Delta t}}{2}+ \frac{{\Delta t\eta_h (\theta )}}{2}}}{{1 + a\frac{{\Delta t}}{{{h^2}}}{{\sin }^2}\frac{\theta }{2} - \frac{{b\Delta t}}{{2h}}\ii\sin \theta  - \frac{{c\Delta t}}{2} - \frac{{\Delta t\eta_h (\theta )}}{2}}} \hat u_i^n
\end{equation}

Note we have
\begin{multline}
{\left| {1 - a\frac{{\Delta t}}{{{h^2}}}{{\sin }^2}\frac{\theta }{2} + \frac{{b\Delta t}}{{2h}}\ii\sin \theta  + \frac{{c\Delta t}}{2} + \frac{{\Delta t\eta_h (\theta )}}{2}} \right|^2} = \\
{\left| {1 - a\frac{{\Delta t}}{{{h^2}}}{{\sin }^2}\frac{\theta }{2} + \frac{{c\Delta t}}{2} + \frac{{\Delta t\eta_h (\theta )}}{2}} \right|^2} + {\left| {\frac{{b\Delta t}}{{2h}}\sin \theta } \right|^2}
\end{multline}
\begin{multline}
\left| {1 + a\frac{{\Delta t}}{{{h^2}}}{{\sin }^2}\frac{\theta }{2} - \frac{{b\Delta t}}{{2h}}\ii\sin \theta  - \frac{{c\Delta t}}{2} - \frac{{\Delta t\eta_h (\theta )}}{2}} \right| = \\
{\left| {1 + a\frac{{\Delta t}}{{{h^2}}}{{\sin }^2}\frac{\theta }{2} - \frac{{c\Delta t}}{2} - \frac{{\Delta t\eta_h (\theta )}}{2}} \right|^2} + {\left| {\frac{{b\Delta t}}{{2h}}\sin \theta } \right|^2}
\end{multline}

Since we have $a\geq 0$, $c\leq 0$, $\eta_h(\theta)\leq  0$, we always have
\begin{equation}
	\left| {1 + a\frac{{\Delta t}}{{{h^2}}}{{\sin }^2}\frac{\theta }{2} - \frac{{c\Delta t}}{2} - \frac{{\Delta t\eta (\theta )}}{2}} \right| \ge \left| {1 - a\frac{{\Delta t}}{{{h^2}}}{{\sin }^2}\frac{\theta }{2} + \frac{{c\Delta t}}{2} + \frac{{\Delta t\eta (\theta )}}{2}} \right|
\end{equation}

Therefore, the model of the ratio in \cref{equ:von} is always no greater than 1. Thus all the wave modes $e^{\ii \theta x}$ will not grow in magnitude if we carry out the Crank-Nicolson scheme. 

\subsubsection{Consistency}
In consideration of \cref{equ:small}, we assume that 
\begin{equation}\label{equ:assumption}
	\nu(y) = 0 \quad y>B_r \mbox{ or } y<B_l
\end{equation}

The consistency is a direct result of the Crank Nicolson scheme. Note that \cref{equ:dl} is the trapezoidal discretization of the nonlocal operator, we have
\begin{equation}
	\int_{B_l}^{B_r} (u(x_j+y)-u(x_j))\nu(y)dy = (\delta_L u)_j + \mathcal{O}(h^2)
\end{equation}
and therefore
\begin{equation}\label{equ:deltal}
  \begin{aligned}
	&\frac{{({\delta _L}u)_j^n + ({\delta _L}u)_j^{n + 1}}}{2} \\
	=& \int_{{B_l}}^{{B_r}} {\left( {\frac{{u({x_{i + j}} + y,{t_n}) + u({x_{i + j}} + y,{t_{n + 1}})}}{2} - \frac{{u({x_i} + y,{t_n}) + u({x_i} + y,{t_{n + 1}})}}{2}} \right)} \nu (y)dy + \mathcal{O}( {h^2})\\
	 =& \int_{{B_l}}^{{B_r}} {\left( {u\left( {{x_{i + j}} + y,{t_{n + \frac{1}{2}}}} \right) - u\left( {{x_i} + y,{t_{n + \frac{1}{2}}}} \right)} \right)} \nu (y)dy + \mathcal{O}(\Delta {t^2} + {h^2})\\
	  =& {\cal L}u\left( {{x_i},{t_{n + \frac{1}{2}}}} \right) + \mathcal{O}(\Delta {t^2} + {h^2})
\end{aligned}
\end{equation}
It is classical results that
\begin{multline}\label{equ:deltar}
	\frac{{(a\delta _x^2 + b{\delta _{2x}} + c)u(x_i,t_n) + (a\delta _x^2 + b{\delta _{2x}} + c)u(x_i,t_{n+1})}}{2} \\= a{u_{xx}}\left( {{x_i},{t_{n + \frac{1}{2}}}} \right) + b{u_x}\left( {{x_i},{t_{n + \frac{1}{2}}}} \right) + cu\left( {{x_i},{t_{n + \frac{1}{2}}}} \right)+\mathcal{O}(h^2+\Delta t^2)
\end{multline}
and that
\[\frac{{u({x_i},{t_{n + 1}}) - u({x_i},{t_n})}}{{\Delta t}} = \mathcal{O}(\Delta t^2)\]
therefore combining \cref{equ:deltal,equ:deltar} we have
\begin{lemma}[Consistency]\label{lemma:consistency}
	Assume \cref{equ:assumption} holds. Then the truncation error for the numerical scheme \cref{equ:scheme}  
	\begin{equation}
		T^n_i := \frac{{u({x_i},{t_{n + 1}}) - u({x_i},{t_n})}}{{\Delta t}} - \frac{{(a\delta _x^2 + b{\delta _{2x}} + c + {\delta _L})u({x_i},{t_n}) + (a\delta _x^2 + b{\delta _{2x}} + c + {\delta _L})u({x_i},{t_{n + 1}})}}{2}
	\end{equation}
	satisfies
	\begin{equation}
		T^n_i = \mathcal{O}(\Delta t^2 + h^2)
	\end{equation}
\end{lemma}


\subsubsection{Convergence}

Finally, we are in a position to prove the convergence of the numerical scheme \cref{equ:scheme}. 

\begin{theorem}
	Assume that $\nu(y)\in C(\RR)$ and the condition in \cref{lemma:consistency} is satisfied. Let $u_i^n$ be the numerical solution at $x_i$ and time $t_n$, and $u(x, t)$ be the exact solution. Then the numerical scheme \cref{equ:scheme} is unconditionally stable and 
	\begin{equation}
		|u(x_i,t_n)-u_i^n| = \mathcal{O}(\Delta t^2 + h^2) \quad \Delta t \rightarrow 0, h\rightarrow 0
	\end{equation}
\end{theorem}
\begin{proof}
	The theorem is a direct result of the stability and consistency. 
\end{proof}

\lib{Singular and Slow Decaying L\'evy Measure}\label{sect:extension}

We now consider the general case where $\nu(y)$ can behave very badly. One such example is the fractional Laplacian where the L\'evy measure is 
\begin{equation}
	\nu(y) = \frac{c_{1,s}}{|y|^{1+2s}}
\end{equation}
where $s\in (0,1)$ and $c_{1,s}$ is a positive constant. Note in this case, $\int_{\RR}(u(x+y)-u(x))\nu(y)dy$ must be understood in the principal value integration. The corresponding stochastic process associated with the fractional Laplacian is the $\alpha$-stable process. 

Consider the general singular integral operator
\begin{equation}\label{equ:Ix}
	I(x) = \int_{\RR}(u(x+y)-u(x)-\rho(y)u'(x)y)\nu(y)dy
\end{equation}
where $\rho(y)u'(x)$ is a drift term to remove small activity from the jumps. $\rho(y)$ is a radial symmetric window function, satisfying
\begin{equation}\label{equ:rho_condition}
\begin{cases}
	1-\rho(y)\sim \mathcal{O}(y^4)& y\rightarrow 0\\
	\rho(y)=0 & |y|\geq r
\end{cases}
\end{equation}
where $r>0$ is a positive number. 

As a reminder, we require $\nu(y)$ to satisfy the following conditions
\begin{equation}\label{equ:ny}
	\int_{-r}^ry^2\nu(y)dy <\infty, \quad \int_{|y|\geq r} \nu(y)dy<\infty
\end{equation} 

The choice of $\rho(y)$ doesn't matter. In fact, if $\tilde\rho(x)$ is another window function that satisfies \cref{equ:rho_condition}, we have
\begin{align}
	&\int_{\RR}(u(x+y)-u(x)-\tilde\rho(y)u'(x)y)\nu(y)dy \\
	=& \int_{\RR}(u(x+y)-u(x)-\rho(y)u'(x)y)\nu(y)dy + u'(x)\int_{\RR}(\rho(y)-\tilde\rho(y)y)\nu(y)dy
\end{align}
we can add the second drift term into the model. 

The first condition in \cref{equ:rho_condition} is designed to take into consideration of the heavy tail case, where $\nu(y)$ can decay like $\mathcal{O}(1/|y|^{1+2s})$ for some $s\in (0,1)$. For example, in the special case
\begin{equation}
	\nu(y) = c_{1,s}\frac{1}{|y|^{1+2s}}
\end{equation}
we obtain the fractional Laplacian. Then $\int_{\RR}(u(x+y)-u(x))\nu(y)dy$ is not well defined but only in the principle value integration, and it is easy to see
\begin{equation}
	\mathrm{p.v.}\int_{\RR}(u(x+y)-u(x))\nu(y)dy = \int_{\RR}(u(x+y)-u(x)-\rho(y)u'(x)y)\nu(y)dy
\end{equation}
for any valid window function $\rho(y)$ thanks to the cancellation of the drift term due to symmetry. 

Although $\int_{\RR}(u(x+y)-u(x)-\rho(y)u'(x)y)\nu(y)dy$ is well-defined in this case, the integrand will behave like 
\begin{equation}
	(u(x+y)-u(x)-\rho(y)u'(x)y)\nu(y) = \mathcal{O}\left(\frac{1}{|y|^{2s-1}}\right)
\end{equation}
in the case $s\rightarrow 0+$, we will have numerical difficulty if a direct numerical integration is applied. In the following, we will propose a numerical discretization for \cref{equ:Ix} targeting at the most numerical challenging case described above 
\begin{equation}\label{equ:ny2}
	\nu(y) = \frac{n_0(y)}{|y|^{1+2s}}
\end{equation} 
where $n_0(y)$ is a bounded continuous function. 

We make two assumptions on $u(x)$
\begin{itemize}
	\item $u\in C(\RR)$ 
	\item Local smoothness. $u\in C^4([x-\delta, x+\delta])$ for some $\delta>0$, i.e., $u$ has fourth order derivative near the location where we want to evaluate $I(x)$.
	\item Far field asymptotic limit. The far field contribution
	\begin{equation}\label{equ:fxy}
		f_x(y) = \int_{\RR}u(x+y)\nu(y)dy
	\end{equation}
	is well defined. In the case $\frac{u(x+y)}{u(y)}\rightarrow f(y)$, this term can be approximated by $\int_\RR f(y)dy$
\end{itemize}

The strategy is to singularity subtraction. We subtract a local diffusion term from \cref{equ:Ix}
\begin{equation}\label{equ:evalIx1d}
	I(x) = \int_{\RR}(u(x+y)-u(x)-\rho(y)u'(x)y- \frac{1}{2}\rho(y)u''(x)y^2 )\nu(y)dy + \frac{1}{2}u''(x)\int_\RR \rho(y)\nu(y)y^2 dy
\end{equation}
We can immediately split the first integral into two parts
\begin{equation}
	I_1(x) = \int_{|y|\leq L_W}(u(x+y)-u(x)-\rho(y)u'(x)y- \frac{1}{2}\rho(y)u''(x)y^2 )\nu(y)dy
\end{equation}
and
\begin{align}
	I_2(x) =& \int_{|y|> L_W}(u(x+y)-u(x)-\rho(y)u'(x)y- \frac{1}{2}\rho(y)u''(x)y^2 )\nu(y)dy\\
	 = &\int_{|x|> L_W}(u(x+y)-u(x) )\nu(y)dy = f_x^{L_W} - u(x)\int_{|y|> L_W}\nu(y)dy
\end{align}
where $L_W$ is a mask window and satisfies
\begin{equation}
	L_W > r
\end{equation}

By Taylor expansion it is easy to see
\begin{equation}
	u(x+y)-u(x)-\rho(y)u'(x)y- \frac{1}{2}\rho(y)u''(x)y^2 =\mathcal{O}(|y|^3)
\end{equation}
and therefore $I_1(x)$ will behave like $\mathcal{O}(|y|^{2-2s})$ near the origin. We will prove this fact in the appendix. That indicates the integrand becomes continuous near the origin. Thus $I_1(x)$ is well defined.   

As $y\rightarrow \infty$, the term terms in $I_2(x)$ are both well defined according to the assumptions \cref{equ:fxy,equ:ny}. 

The second term in \cref{equ:evalIx1d} 
\begin{equation}
	I_3(x) = \frac{1}{2}u''(x)\int_{|y|\leq r}\nu(y)y^2 dy
\end{equation}
is a local diffusion term and the coefficient is well defined according to \cref{equ:ny}.

We now focus on the numerical discretization of $I_1(x)$, $I_2(x)$ and $I_3(x)$. We divide the mask window into $2N$ uniform subintervals and consider the grid $\{ih:i\in\mathbb{Z} \}$, where $h = L_W/N$. We denote $u_i = u(ih)$.

Since the integrand in $I_1(x)$ is continuous, we can use a simple trapezoidal quadrature rule to approximate the integral. Assume the quadrature weights are $w_j$ given by $w_{-N}=w_N = \frac{h}{2}$ and $w_j=h$, $j=-N+1, -N+2, \ldots, N-2, N-1$.
\begin{equation}\label{equ:I1approx}
  \begin{aligned}
	I_1(x_i) &\approx \sum\limits_{j =  - N}^N {{u_{i + j}}n(jh){w_j}}  - {u_i}\sum\limits_{j =  - N}^N {n(jh){w_j}} \\
	& - \frac{{{u_{i + 1}} - {u_{i - 1}}}}{h}\sum\limits_{j =  - N}^N {\rho (jh)n(jh)jh}  - \frac{{{u_{i + 1}} + {u_{i - 1}} - 2{u_i}}}{{2{h^2}}}\sum\limits_{j =  - N}^N {\rho (jh){{(jh)}^2}n(jh)} 
\end{aligned}
\end{equation}
where $\sum\limits_{j =  - N}^N$ denotes the summation excluding $j=0$. 

For $I_2(x)$, $f_x^{L_W}$ is either provided as an input or computed using a numerical quadrature and so is $\int_{|x|>L_W} \nu(y)dy$. We will see how these terms are obtained in the examples below. 
\begin{equation}\label{equ:I2approx}
	I_2(x_i)\approx f_{x_i}^{L_W} - u(x_i)\int_{|y|> L_W}\nu(y)dy
\end{equation}
For $I_3$, a central difference scheme is applied to the second order derivative term. 
\begin{equation}\label{equ:I3approx}
	I_3(x_i)\approx \frac{{{u_{i + 1}} + {u_{i - 1}} - 2{u_i}}}{{2{h^2}}}\int_{|y| \leqslant r} n (y){y^2}dy
\end{equation}
and the integral can either be computed analytically or numerically. 


In practice, we want to compute $I(x)$ for $x\in [-L,L]$, according to \cref{tab:1d}, we need to know the values of $u(x)$ on $[-L-L_W,L+L_W]$ and its corresponding far-field interactions. \Cref{fig:fig31} visualizes the relationship. To compute $I(x_i)$, we need to compute the near field interaction and local interaction using values of $u(x)$ from the green area. The values of $u(x)$ are provided in the green and red area for computing $I(x)$, $x\in [-L, L]$.

\begin{figure}[H] % \usepackage{float}
\centering
\includegraphics[width=0.8\textwidth,keepaspectratio]{figures/fig31}
\caption{To compute $I(x_i)$, we need to compute the near field interaction and local interaction using values of $u(x)$ from the green area. The values of $u(x)$ are provided in the green and red area for computing $I(x)$, $x\in [-L, L]$.}
\label{fig:fig31}
\end{figure}


Although we have used a different formula for the evaluation of the integral, we should soon realize that in the far-away off-diagonal parts, the entries are still $\nu_j h$~(except on the boundary), which the $\mathcal{H}$ matrix construction routine can still work. 

\lib{Applications}

In this section, we list several possible applications of the numerical scheme and fast algorithms. These applications are taken from literature which can be formulated as an integro-differential equation.

\subsection{Option Pricing}

Let $S_t$ be the price of a financial asset which is modelled as a stochastic process under a martingale equivalence measure $\mathbb{Q}'$ and on a filtered probability space $(\Omega, \mathcal{F}, \mathcal{F}_t, \mathbb{Q})$.

One of the popular models is the exponential L\'evy model which assumes
\begin{equation}
	S_t = S_0 e^{rt+X_t}
\end{equation}
where $X_t$ is a L\'evy process. Assume $r$ is the interest rate. For a European call or put, the terminal payoff $H_T$ at time $T$ is associated with the underlying asset price $S_T$
\begin{equation}
	H_T = H(S_T)
\end{equation}
 The value of the option is defined as a discounted conditional expection of $H_T$ under the risk-adjusted martingale measure
\begin{equation}
	C_t = \mathbb{E}[e^{-r(T-t)}H(S_T)|\mathcal{F}_t] = \mathbb{E}[e^{-r(T-t)}H(S_T)|S_t=S]
\end{equation}

By introducing $\tau = T-t$, $x=\log\left( \frac{S}{S_0} \right)$, and define
\begin{equation}
	u(x,\tau) = \mathbb{E}[h(x+Y_\tau)]\quad h(x) = H(S_0 e^x)
\end{equation}
 for sufficiently smooth $u$, by applying the Ito's formula for L\'evy process we have the integro-differential equation
\begin{equation}
	\frac{\partial u}{\partial \tau} = \frac{\sigma^2}{2}u_{xx} - (\sigma^2/2-r+\alpha) u_x + \int_\RR (u(x+y)-u(x)-u'(x)\mathbf{1}_{0<|y|<1}(y)y)\nu(y)dy
\end{equation}
with initial condition
\begin{equation}
	u(0,x) = h(x)
\end{equation}


\subsection{Quantum Mechanics}

If the underlying stochastic process powering the random fluctuations is a Gaussian Brownian motion, we obtain the non relativistic Schr\"odinger's equation
\begin{equation}
	\ii \hbar \partial_t \psi(x,t) = -\frac{\hbar^2}{2m}\partial^2_{x} \psi(x,t)
\end{equation}

In recent years, there are a growing interest in non Gaussian stochastic process, and particularly the L\'evy process. One of the popular models is the fractional quantum mechanics, where the stable processes are used as the underlying stochastic process. The popularity of the stable process is justified by the properties of scaling and self-similarity displayed by the process. For any distribution with power-law decay $\frac{1}{|x|^{1+\alpha}}$, $0<\alpha<1$ the generalized central limit theorem guarantee that their sum scaled by $\frac{1}{n^{1/\alpha}}$ converge to the $\alpha$-stable distribution. If the variance are finite, i.e., $\alpha\geq 2$, then the central limit theorem holds, where  their sum scaled by $\frac{1}{n^{1/2}}$, properly centered, and identically distributed, converge to the Gaussian distribution.  This leads to the fractional Schr\"odinger equation
\begin{equation}
	\ii \hbar \partial_t \psi(x,t) = D_\alpha (-\hbar^2 \Delta)^{\alpha/2}\psi(x,t)
\end{equation}
where $(-\hbar^2 \Delta)^{\alpha/2}$ is the fractional Laplacian which can be defined through
\begin{equation}
	(-\hbar^2 \Delta)^{\alpha/2}\psi(x,t) = \frac{1}{(2\pi \hbar)^3}\int |\xi|^\alpha \hat \psi(\xi,t) \exp(\ii (\xi,x)/\hbar)d\xi
\end{equation}

\url{https://pdfs.semanticscholar.org/1dc0/9fedfa72c9f051da4642ff753d2bfe9f7a5f.pdf?_ga=2.231665951.1050874845.1539891109-5919569.1537093630}

More generally, other L\'evy measure can be used to develop the quantum mechanics. The more general Schr\"odinger equation reads
\begin{equation}
	\ii \hbar\partial_t \psi(x,t) = -\frac{\hbar^2}{2m}\partial_x^2 \psi(x,t) - \hbar  \int_\RR [\psi(x+y,t)-\psi(x,t)]\nu(y)dy
\end{equation}

\url{https://arxiv.org/pdf/0805.0503.pdf}

Some examples of the L\'evy-Schr\"odinger equations are
\begin{itemize}
	\item Relativistic.
	\begin{equation}
		\ii \hbar\partial_t \psi(x,t) = \sqrt{m^2c^4-c^2\hbar^2\partial_x^2} \psi(x,t)
	\end{equation}
	\item Variance-Gamma laws
	\begin{equation}
		\ii \hbar\partial_t \psi(x,t) = -\frac{\lambda\hbar}{\tau}\int_\RR\frac{\psi(x+y,t)-\psi(x,t)}{|y|}e^{-|y|/\hbar}dy
	\end{equation}

\end{itemize}

\subsection{Turbulence Flow}

\url{https://arxiv.org/pdf/1803.05286.pdf}

It is known that turbulence flow exhibits anomalous diffusion, i.e., the diffusion occurs over distance $\xi$ may scale more than one half, $\xi\sim\mathcal{O}(t^{1/2})$. There are many efforts to model turbulence and capture these anomalies. One of the recent research is the the modeling of turbulence flow via the fractional Laplacian. 

If we assume that the equilibrium probability distribution of particle speeds to be L\'evy $\alpha$-stable distributions instead of the Maxwell-Boltzmann distribution, we will arrive at the Navier-Stokes equation with the fractional Laplacian operator as a means to represent the mean friction force arising in a turbulence flow
\begin{equation}
	\rho\frac{D\bar u}{Dt} = -\nabla p + \mu_\alpha \nabla^2 \bar u + \rho C_\alpha \int_{\RR^3} \frac{\bar u(x',t)-\bar u(x,t)}{|x-x'|^{\alpha+3}} dx'
\end{equation}



\lib{Numerical Examples}


\subsection{Efficiency of $\mathcal{H}$-Matrix: 1D Case}

In this section, we show the efficiency of the arithmetic operations using the $\mathcal{H}$-matrix in 1D. In the experiment, the minimum block size is either 64 or 32. The matrix sizes tested are $2^5, 2^6, \ldots, 2^{17}$. The maximum block size for $2^n\times 2^n$ matrices is $2^{n-3}\times 2^{n-3}$. The rank for off-diagonal approximation is 10, which is quite accurate for the Merton kernel we considered. The author observed that for $2^{17}\times 2^{17}$ matrices, the dense LU will throw \texttt{OutOfMemory} error. The codes are implemented using \texttt{julia}. 


We divide the interval $[-1,1]$ into $2^n$ equal length intervals, $h=\frac{1}{2^{n-1}}$. Consider the following kernel function for the construction of $\mathcal{H}$-matrix
\begin{equation}
	k(x,y) = \begin{cases}
		-\frac{10}{h} + e^{-|x-y|^2} & \mbox{ if } x=y\\
		e^{-|x-y|^2} & \mbox{ otherwise}
	\end{cases}
\end{equation}
the term $-\frac{10}{h}$ is used to adjust the condition number of the matrix~(it also arises naturally from implicit scheme of the PDE derived from L\'evy process). For admissibility condition,  we use $\eta=1$. For low-rank blocks, the rank is fixed to be 5. In effect, the rank can be chosen adaptively; however, we observe that the fixed rank strategy is practical for our cases. 

A typical hierarchical matrix in 1D will have the skeleton shown in \cref{fig:1D}. Here we use a different color for each block. The blue and green blocks represent low rank blocks while the yellow blocks represent dense blocks. The matrix is arranged into a hierarchical structure, from which $\mathcal{H}$-matrix got its name. 

\begin{figure}[H] % \usepackage{float}
\centering
\includegraphics[width=0.5\textwidth,keepaspectratio]{figures/1D}
\caption{We use a different color for each block. The blue and green blocks represent low rank blocks while the yellow blocks represent dense blocks.}
\label{fig:1D}
\end{figure} 


The key for maintaining optimal rates while the problem size $N$ becomes large is to control the total number of dense blocks. In principal, the number of dense blocks should grow linearly with problem size, which can be demonstrated by looking at the compression ratio or the number of total blocks~(full dense blocks as well as low rank blocks). 


In \cref{fig:compress}, the left figure shows the compression ratio of the $\mathcal{H}$-matrix, which is defined as
\begin{equation}
\texttt{CompressRatio}=\frac{\#\texttt{Matrix Entries Stored}}{\#\texttt{Full Matrix Entries}}
\end{equation}
compress ratio gives us an estimate of the efficiency of the $\mathcal{H}$-matrix storage. Small compress ratios for large matrices is critical for large scale problems. On the right hand size, we show that number of full subblocks and low-rank subblocks as $N$ grows. The linear growth is easily read off the figure. 

\begin{figure}[htpb]
\centering
\scalebox{0.4}{\input{figures/compress.pgf}}~
\scalebox{0.4}{\input{figures/icme_structure.pgf}}~
\caption{Compression ratio and number of blocks as the problem size grows.}
\label{fig:compress}
\end{figure}



\paragraph{Matrix Vector Multipliction}

\Cref{fig:matvec} shows the complexity of the matrix vector multiplication for full matrices and $\mathcal{H}$-matrix. Compared to the dense matrix vector multiplication, the $\mathcal{H}$-matrix structure lends us great speedup. This enables us to device highly efficient iterative solvers, such as preconditioned conjugate gradient method, which may requires many matrix vector productions during the iterations.  

\begin{figure}[htpb]
\centering
\scalebox{0.6}{\input{figures/icme_matvec.pgf}}
\caption{Matrix vector production is also much more efficient using the $\mathcal{H}$-matrices than using the dense matrix. It has the asymptotic complexity rate slightly above $\mathcal{O}(N)$, compared to $\mathcal{O}(N^2)$ for dense matrices.}
\label{fig:matvec}
\end{figure}


\paragraph{LU Decomposition}

We have already shown that the storage and construction complexity is $\mathcal{O}(N)$ in \cref{fig:construction}. In \cref{fig:lu} we also show that the LU decomposition is also much more efficient using the $\mathcal{H}$-matrices. We see that the $\mathcal{H}$-LU has better asymptotic complexity than the dense LU, which has complexity $\mathcal{O}(N^3)$. Note the $\mathcal{H}$-LU decomposition is carried out using high accuracy and can serve as a direct solver for linear systems. We need to point out that although the $\mathcal{H}$-LU tends to beat dense LU in terms of time consumption for large scale problems, the constant in the asymptotic rate $\mathcal{O}(N)$ is still large, which is well-known in literatures. Also, since we have fixed ranks, the practical time consumption is not optimal and has a higher asymptotic rate than the linear one. 

\begin{figure}[htpb]
\centering
\scalebox{0.6}{\input{figures/icme_lu.pgf}}
\caption{LU decomposition of $\mathcal{H}$-LU and the dense LU. The $\mathcal{H}$-LU has better asymptotic complexity than the dense LU, which has complexity $\mathcal{O}(N^3)$.}
\label{fig:lu}
\end{figure}



\paragraph{Solve}

One crucial step for a successful implicit scheme is to solve the equation $A\bx=\by$. We can of course use matrix-free solvers such as PCG. However, in the case that $A$ is ill-conditioned we may require a good preconditioner. Finding such a preconditioner is not an easy task, especially for the dense matrices, which is not covered by literature as comprehensively as that of sparse counterparts. $\mathcal{H}$-LU lends us a generic way to construct preconditioners or direct solvers. In both cases, we need to factorize $A$ as mentioned, and then solve $A\bx=\by$. 

In this numerical experiment, we generate a random vector and record the solving time for both factorized $\mathcal{H}$-matrix and LU factorized dense matrix. In \Cref{fig:solve} compare the solving time for both the dense matrix and the $\mathcal{H}$-matrix. We see that the $\mathcal{H}$-matrix solving is both faster and has better asymptotic rate than the dense one.

\begin{figure}[htpb]
\centering
\scalebox{0.6}{\input{figures/solve.pgf}}
\caption{Solving time for both the dense matrix and the $\mathcal{H}$-matrix. The $\mathcal{H}$-matrix solving is both faster and has better asymptotic rate than the dense one.}
\label{fig:solve}
\end{figure}

\subsection{Efficiency of $\mathcal{H}$-Matrix: 2D Case}

We briefly mention that the $\mathcal{H}$-matrix technique also works well in 2D. In this case, we divide $[-1,1]^2$ into $2^n\times 2^n$ equal size squares and let $h=\frac{1}{2^{n-1}}$. We use the kernel function
\begin{equation}\label{equ:k}
	k(\bx,\by) = \begin{cases}
		-\frac{10}{h^2} + e^{-|\bx-\by|^2} & \mbox{ if } \bx=\by\\
		e^{-|\bx-\by|^2} & \mbox{ otherwise}
	\end{cases}
\end{equation}
We use the rank $r=5$. A typical $\mathcal{H}$-matrix is shown in \cref{fig:2d}.

\begin{figure}[H] % \usepackage{float}
\centering
\includegraphics[width=0.5\textwidth,keepaspectratio]{figures/2D}
\caption{A typical $\mathcal{H}$-matrix in 2D constructed from \cref{equ:k}.}
\label{fig:2d}
\end{figure}


We perform the same comparison as that in the last section. The $\mathcal{H}$-matrix technique has better asymptotic rate than that of the dense matrices in terms of time consumption. $\mathcal{H}$-matrix will have great advantage over the dense matrices over the dense matrices for large scale problems. 
\begin{figure}[htpb]
\centering
\scalebox{0.6}{\input{figures/dense.pgf}}
\caption{Comparison of construction time, matrix vector multiplication time, LU decomposition, and solving for both dense matrices as well as $\mathcal{H}$-matrices. The $\mathcal{H}$-matrix technique has better asymptotic rate than that of the dense matrices in terms of time consumption. The green dashed line shows the theoretical complexity asymptotic rate for dense matrices.}
\label{fig:solve}
\end{figure}


\subsection{Convection Diffusion Equation Driven by the L\'evy Process}

In this section, we consider solving the model problem using $\mathcal{H}$-matrix and full matrix. We use a large rank $5$ for the kernel function and therefore the $\mathcal{H}$ matrix as well as its LU decomposition are very accurate. Thus, we do not need iterative solvers. Instead we can directly do triangular solve given the LU decomposition at each iteration. Another choice is to use the LU decomposed matrix as preconditioners, which we will not pursue here.  

\Cref{fig:s} shows the computational cost as well as the storage cost for both $\mathcal{H}$-matrix and the full matrix. We see that both in terms of time and storage complexity $\mathcal{H}$-matrix has advantage over full matrices.

\begin{figure}[htpb]
\centering
\scalebox{0.3}{\input{figures/s1.pgf}}~
\scalebox{0.3}{\input{figures/s2.pgf}}

\caption{Time and storage complexity for $\mathcal{H}$-matrix and full matrix. Both in terms of time and storage complexity $\mathcal{H}$-matrix has advantage over full matrices for large scale problems.}
\label{fig:s}
\end{figure}



\paragraph{Verification Method} We briefly mention how we verify our numerical solution. We compare the numerical solution to the analytical solution. The analytical solution to the model problem \cref{equ:ut} can be found using the Fourier transform when the coefficients are constant. For simplicity, we consider the case where the jump distribution is the Gaussian distribution, i.e. 
\begin{equation}
	\nu(y) = d\exp(-\varepsilon^2 y^2)
\end{equation}
Then the corresponding characteristic function of the L\'evy measure is 
\begin{equation}
	\eta(\xi) = -a\xi^2 + b\ii \xi + c + \int_\RR (e^{\ii \xi y}-1)\nu(y)dy
\end{equation}

The last term can be computed analytically,
\begin{align}
	\int_{\RR} {({e^{\ii\xi y}} - 1)} \nu (y)dy =& d\int_{\RR} {{e^{\ii\xi y}}\nu (y)} dy - d\frac{{\sqrt \pi  }}{\varepsilon }\\
	 =&\frac{{ d\sqrt \pi  }}{\varepsilon }\left( {{e^{ - \frac{{{\xi ^2}}}{{4{\varepsilon ^2}}}}} - 1} \right)
\end{align}

Therefore, from 
\begin{equation}
	\hat u_t(\xi, t) = \eta(\xi) \hat u(\xi, 0)
\end{equation}
we obtain
\begin{equation}
	\hat u(\xi, t) = \exp(\eta(\xi)t)\hat u(\xi, 0)
\end{equation}

The values of $u(x,t)$ at time $t=T$ can be evaluated through inverse Fourier transform
\begin{equation}
	u(x_i, T) = \frac{1}{\sqrt{2\pi}}\int_\RR \hat u(\xi, T) e^{\ii \xi x_i} d\xi  = \frac{1}{\sqrt{2\pi}}\int_\RR  \exp(\eta(\xi)t)\hat u(\xi, 0) e^{\ii \xi x_i} d\xi 
\end{equation}
which can be evaluated using FFT for uniform grids $x_i$. 

\subsection{Singular and Slow Decaying L\'evy Measure}

Finally, we consider the case where $\nu(y)$ grows to infinity at $y=0$ and has heavy tail. The case is quite challenging and extensively studied by the community nowadays. For simplicity, we will consider the specific case where $n(\bx) = \frac{C_{d, s}}{|\bx|^{d+2s}}$, i.e., the fractional Laplacian. Fortunately we can compute  the analytical nonlocal derivative or gradient for some functions. 

For the first example, we consider $u(x)=\exp(-x^2)$ in 1D. Then we have
\begin{equation}\label{equ:u0}
	(-\Delta)^s u(0) = 2^{2s}\Gamma\left( \frac{1+2s}{2} \right)/\sqrt{\pi}
\end{equation}

For this example, since $u(x)$ decay to zero exponentially, we can assume that the far-field interaction $f^{L_W}_x=0$ is zero for sufficiently large $L_W$. The numerical value is computed using \cref{equ:I1approx,equ:I2approx,equ:I3approx} and compared with the exact value \cref{equ:u0}. The parameters are: $L_W=5.0$, $r=0.2$. The convergence plot is shown in \cref{fig:fig5}. We can see that the error converges like or better than $\mathcal{O}(h^2)$. 

\begin{figure}[H] % \usepackage{float}
\centering
\includegraphics[width=0.7\textwidth,keepaspectratio]{figures/fig5}
\caption{Numerical error for approximating \cref{equ:u0}. The error converges like or better than $\mathcal{O}(h^2)$.}
\label{fig:fig5}
\end{figure}

We also test the scheme on a challenging problem: the fractional Poisson problem. The PDE
\begin{equation}
	\begin{cases}
		(-\Delta)^s u(x) = 1 & x\in [-1,1]\\
		u(x) = 0 & x\not\in [-1,1]
	\end{cases}
\end{equation}
has a unique solution
\begin{equation}
	u(x) = \frac{2^{2s}\Gamma(1+s)\Gamma\left( \frac{1+2s}{2} \right) }{\Gamma(1/2)} (1-x^2)^s
\end{equation}
Note that $u(x)$ is not smooth across the boundary. In fact, it only belongs to $C^{0,s}([-1,1])$, the $s$-order H\"older space. Numerical algorithms usually exhibit reduced convergence.  We use $L=1.0$ and $Lw=2.0$ so that the support of $u(x)$ is included in the near-field or local interaction. Thus we have $f_x^{L_W}=0$. Since the current implementation only supports forward computation of the nonlocal operator, i.e., given function values, the nonlocal derivative or gradient is computed, we resort to a conjugate gradient approach for recovering $u(x)$ in $[-L,L]$. 

\Cref{fig:fig6} presents the finite difference result obtained from our discretization. We can see that the convergence order is $1.0$ or less, much worse than the Poisson problem where $\mathcal{O}(h^2)$ convergence rate is typical. We need to emphasize this is a universal problem faced by many fractional Laplacian models if a simple truncation trick is used. 

\begin{figure}[H] % \usepackage{float}
\centering
\includegraphics[width=0.45\textwidth,keepaspectratio]{figures/fig6}
\includegraphics[width=0.45\textwidth,keepaspectratio]{figures/fig7}
\caption{The finite difference result obtained from our discretization. The convergence order is $1.0$ or less, much worse than the Poisson problem where $\mathcal{O}(h^2)$ convergence rate is typical}
\label{fig:fig6}
\end{figure}

Finally, we also consider the computation of $(-\Delta)^ u(\bx)$ in 2D, where
\begin{equation}
	u(\bx) = \frac{1}{2^{2s}\Gamma(1+s)^2} ((1-|\bx|^2)^s_+
\end{equation}

The analytically result is known for $|\bx|\leq 1$, which is
\begin{equation}\label{equ:u1}
	(-\Delta)^s u(\bx) = 1\quad |\bx|\leq 1
\end{equation}
The numerical result is shown in \cref{fig:fig8}. Near the boundary, due to the nonsmoothness of $u(\bx)$, the algorithm is having a hard time computing the nonlocal gradient and therefore we see the oscillatory behavior. However, the computation for the region near the center is  good, which does not suffer much from the far-away contribution from nonsmooth boundaries. In the center the error is only $4\%$. We used $L_W=2.0$ and $L=1.0$ in this case.  

\begin{figure}[H] % \usepackage{float}
\centering
\includegraphics[width=0.8\textwidth,keepaspectratio]{figures/fig8}
\caption{Numerical evaluation of \cref{equ:u1}. Near the boundary, due to the nonsmoothness of $u(\bx)$, the algorithm is having a hard time computing the nonlocal gradient and therefore we see the oscillatory behavior. However, the computation for the region near the center is  good, which does not suffer much from the far-away contribution from nonsmooth boundaries. In the center the error is only $4\%$.}
\label{fig:fig8}
\end{figure}

These numerical examples demonstrates that the numerical scheme also works for $\nu(y)$ which has heavy tails. 


\section{Conclusion}

In this paper, we presented the $\mathcal{H}$-matrix solver for the convection diffusion equation driven by the L\'evy process. We consider both semi-heavy L\'evy measure $\nu(y)<\infty$ as well as the challenging case $\nu(y)\rightarrow 0$, $y\rightarrow 0$, and $\nu(y)$ decays only algebraically. Particularly, when $\nu(y) = \frac{c_{1,s}}{|y|^{1+2s}}$, we recover the so-called fractional Laplacian operator $(-\Delta)^s u(x) =\mathrm{r.v.} \int_{\RR} (u(x+y)-u(x))\nu(y)dy$. In the case $\nu(y)$ is smooth for large $y$, the corresponding coefficient matrices in the explicit or implicit scheme can be efficiently represented by $\mathcal{H}$-matrix. We implemented $\mathcal{H}$-LU and use it as a preconditioner or a direct solver for the convection diffusion equation. Numerical methods demonstrate that the $\mathcal{H}$-matrix is highly efficient compared to the dense matrices for these tasks. 

The algorithms proposed in this paper can also be easily generalized to higher dimensions. To demonstrate, we also present the two dimensional cases in this paper, which also shows advantage over direct methods, especially for large scale problems. 

The convection diffusion equation, or other counterparts driven by L\'evy process is challenging due to the non-locality of the jump diffusion. This will lead to dense coefficients matrices which makes computation prohibitable for large scale problems. However, the main finding in this paper shows that by adopting the well-established $\mathcal{H}$-matrix technique, large scale simulation becomes possible and efficient. It could be a first-hand method under researcher's consideration.    

\end{document}